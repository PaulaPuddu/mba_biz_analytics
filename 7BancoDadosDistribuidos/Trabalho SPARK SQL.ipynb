{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBS02116-TBABD-T10_T_BDD 17-24_0322\n",
    "\n",
    "Nome da Equipe: Ana Paula Puddu, Fabio Monteiro, Lucas Sena, Marcos Soares, Wagner Fonseca\n",
    "\n",
    "Banco de Dados Distribuídos_0433_Eq1\n",
    "Biblioteca: Spark SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Descrever as principais funções da biblioteca foco do trabalho:\n",
    "\n",
    "O Apache Spark é uma ferramenta utilizada para processamento de dados em larga escala e estende o modelo de programação MapReduce, muito utilizado por desenvolvedores que utilizam o Hadoop MapReduce. \n",
    "\n",
    "Esta ferramenta de Big Data surgiu por volta de 2009, na Universidade da Califórnia em Berkeley e tem como principal característica a computação cluster em memória, permitindo um processamento de dados muito mais ágil. Além disso, suporta várias linguagens de programação e permite análises mais avançadas.\n",
    "\n",
    "O Apache Spark é uma ferramenta utilizada para processamento de dados em larga escala e estende o modelo de programação MapReduce, muito utilizado por desenvolvedores que utilizam o Hadoop MapReduce. Esta ferramenta de Big Data surgiu por volta de 2009, na Universidade da Califórnia em Berkeley e tem como principal característica a computação cluster em memória, permitindo um processamento de dados muito mais ágil. Além disso, suporta várias linguagens de programação e permite análises mais avançadas.\n",
    "\n",
    "Dentre os diversos módulos do Apache Spark, há o Spark SQL, que permite o acesso a dados estruturados, utilizando a linguagem SQL ANSI. Além disso, o Spark SQL fornece algumas vantagens na sua aplicação:\n",
    "    • Integração com fontes de dados;\n",
    "    • Acesso unificado aos dados(distribuídos);\n",
    "    • Padrão de conectividade (JDBC/ODBC) a fontes de dados;\n",
    "    • Escalabilidade;\n",
    "    • Otimização de performance do acesso aos dados;\n",
    "    • Processamento em batch (hive tables).\n",
    "\n",
    "O Spark SQL fornece dois conjuntos de funções para suprir as necessidades de manipulação dos dados: funções internas(pré-definidas) e funções definidas por usuários (UFD).\n",
    "As funções internas do Spark SQL se dividem em dois grupos, conforme abaixo:\n",
    "    • Funções escalares – para manipulação de matrizes, mapas, data/hora e JSON;\n",
    "    • Funções do tipo Agregado – para agregação, agrupamento de dados e janelas(window)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Descrever os conceitos de RDD e DAG\n",
    "\n",
    "O RDD (Resilient Distributed Dataset) é a estrutura fundamental do Spark. São, em resumo, coleções de dados distribuídos imutáveis (read only), de qualquer tipo. A palavra “Resiliente”, remete a tolerância a falhas, já que os dados estão armazenados em múltiplos nós.\n",
    "\n",
    "A sequência de processamento é o que é chamado de DAG, sigla para Directed Acyclic Graph, ou Grafos acíclicos dirigidos. Em outras palavras, o DAG é uma sequência de execução do processo, e esse processo tem que ser direcionado, ou seja, obedecer a uma sequência de passos. E não pode ser cíclico, quer dizer, não pode ter recursividade ou interatividade entre as etapas de processamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)Descrever os conceitos dos tipos de operações (transformações e ações)\n",
    "\n",
    "É possível ter nos RDDs dois métodos de operações que são agrupados em blocos: Transformações e Ações.\n",
    "\n",
    "Os blocos de transformações são os comandos executados para leitura e preparação dos dados. Essas operações não retornam um valor. Podem ser classificadas neste método os comandos de leitura, filtragem e mapeamento de atributos de um determinado RDD. Após cada um desses comandos, sempre será gerado um novo RDD, já com as transformações aplicadas. \n",
    "\n",
    "Já nos blocos de ações, os comandos executados retornam valores, gravam e executam todos os comandos compostos no DAG. Em outras palvras, quando uma função de ação é executada em um objeto RDD, todas as consultas são executadas e gera um valor retornado. Exemplos de comandos de ações são: count, reduce, take, collect, entre outros.\n",
    "\n",
    "Segue tabelas com algumas das transformações e ações do Spark\n",
    "\n",
    "TRANSFORMAÇÕES\n",
    "\n",
    "map(func) Retorna um novo conjunto de dados distribuído formado passando cada elemento da fonte por meio de uma função func.\n",
    "\n",
    "filter(func) Retornar um novo conjunto de dados formado pela filtragem dos elementos da fonte em que func retorna verdadeiro.\n",
    "\n",
    "flatMap(func) Similar ao map, mas cada item de entrada pode ser mapeado para 0 ou mais itens de saída (então func  deve retornar um sequência ao invés de um único item).\n",
    "\n",
    "join(outro Dataset,[numTasks])Quando executado em conjuntos de dados do tipo ( K, V) e (K, W), retorna um conjunto de dados com todos os pares de elementos para cada chave (K, (V, W)).\n",
    "\n",
    "Outer Joins são suportados através leftOuterJoin, rightOuterJoin e fullOuterJoin.\n",
    "\n",
    "reduceByKey(func[numTasks])Quando executado em um conjunto de dados de pares (k, V ), retorna um conjunto de dados de pares (K, V) onde os valores para cada chave são agregados usando a função de redução func, que deve ser do tipo (V, V) => V.\n",
    "\n",
    "sortByKey([ascending],[numTasks])Quando executado em um conjunto de dados de pares (K, V), onde K implementa a ordenação, retorna um conjunto de dados de pares (K, V) ordenados pela chave\n",
    "\n",
    "AÇÕES\n",
    "\n",
    "collect() Retorna todos os elementos do conjunto de dados como uma matriz no driver do programa. Isto é geralmente útil depois de um filtro, ou outra operação que retorna uma pequeno subconjunto dos dados.\n",
    "\n",
    "count() Retorna o número de elementos no conjunto de dados.\n",
    "\n",
    "first() Retorna o primeiro elemento do conjunto de dados (similar ao take(1)).\n",
    "\n",
    "take(n) Retorna um array com os primeiros n elementos do conjunto de dados.\n",
    "\n",
    "saveAsTextFile(path) Grava os elementos do conjunto de dados como um arquivo de texto (ou conjunto de arquivos de texto ) em um determinado diretório no sistema de arquivos local,HDFS ou qualquer outro sistema de arquivos Hadoop - suportada. Spark vai chamar toString em cada elemento para convertê-lo para uma linha de texto no arquivo.\n",
    "\n",
    "foreach(func) Executa uma função func em cada elemento do conjunto de dados. Isto é normalmente feito para efeitos secundários, tais como a atualização um acumulador ou interação com os sistemas de armazenamento externos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)Apresentar casos de uso da biblioteca\n",
    "\n",
    "A biblioteca SPARK SQL é uma excelente alternativa para facilitar o acesso de um banco de dados distribuido para os principais agentes de uma organização. Tendo em vista a complexidade de se trabalhar com um banco de dados distribuido de volume extraordinário.\n",
    "Com essa integração, diversos departamentos podem consultar informações valiosas dentro do banco de dados através de querys simples, além da possibilidade de criar views restritas que permitem que cada departamento tenha acesso somente ao que é de sua utilidade, aumentando a segurança dos dados e facilitando a analise e processamento das informações. \n",
    "\n",
    "Essa biblioteca seria de grande utilidade para departamentos comerciais de grandes corporações como AMBEV para acessar rapidamente qual o produto mais vendido no ultimo trimeste, tendo em vista o vasto leque de produtos comercializados e o tamanho do market share global da Ambev, essa pergunta simples poderia ter uma enorme complexidade para ser solucionada e com essa biblioteca, os profissionais conseguiriam consultar ao passo de poucas linhas de código na linguagem SQL. \n",
    "\n",
    "Há avanços reais na implementação dessa ferramenta em projetos da Administração Pública Federal, Programas como Bolsa Família geram um volume extraordinário de dados, tendo em vista o número de famílias que utilizam o programa e seu tempo de existência. Para problemas dessa natureza o SPARK SQL se apresenta como uma ótima ferramenta visto que entrega as vantagens de um banco de dados distribuidos com a otimização de consultas para acompanhamento eficiente da execução do peograma.\n",
    "\n",
    "Em termos geral, essa biblioteca visa otimizar consultas a banco de dados de distribuidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)Apresentar uma aplicação utilizando a ferramenta jupyter notebook da VM disponibilizada em aula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/tmp/VMwareDnD/h1u9jA/stores_sales.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark.sql \n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sdf = sqlContext.createDataFrame(df)\n",
    "sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.saveAsTable(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.registerTempTable(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------+\n",
      "|Store|Dept|      Date|Weekly_Sales|\n",
      "+-----+----+----------+------------+\n",
      "|    1|   1|05/02/2010|     24924.5|\n",
      "|    1|   1|12/02/2010|    46039.49|\n",
      "|    1|   1|19/02/2010|    41595.55|\n",
      "|    1|   1|26/02/2010|    19403.54|\n",
      "|    1|   1|05/03/2010|     21827.9|\n",
      "|    1|   1|12/03/2010|    21043.39|\n",
      "|    1|   1|19/03/2010|    22136.64|\n",
      "|    1|   1|26/03/2010|    26229.21|\n",
      "|    1|   1|02/04/2010|    57258.43|\n",
      "|    1|   1|09/04/2010|    42960.91|\n",
      "|    1|   1|16/04/2010|    17596.96|\n",
      "|    1|   1|23/04/2010|    16145.35|\n",
      "|    1|   1|30/04/2010|    16555.11|\n",
      "|    1|   1|07/05/2010|    17413.94|\n",
      "|    1|   1|14/05/2010|    18926.74|\n",
      "|    1|   1|21/05/2010|    14773.04|\n",
      "|    1|   1|28/05/2010|    15580.43|\n",
      "|    1|   1|04/06/2010|    17558.09|\n",
      "|    1|   1|11/06/2010|    16637.62|\n",
      "|    1|   1|18/06/2010|    16216.27|\n",
      "+-----+----+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geral = sqlContext.sql(\"SELECT * FROM sales\")\n",
    "geral.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|count(1)|store|\n",
      "+--------+-----+\n",
      "|   10474|   13|\n",
      "|   10315|   10|\n",
      "|   10272|    4|\n",
      "|   10244|    1|\n",
      "|   10238|    2|\n",
      "|   10228|   24|\n",
      "|   10225|   27|\n",
      "|   10224|   34|\n",
      "|   10214|   20|\n",
      "|   10211|    6|\n",
      "|   10202|   32|\n",
      "|   10148|   19|\n",
      "|   10142|   31|\n",
      "|   10113|   28|\n",
      "|   10088|   41|\n",
      "|   10062|   11|\n",
      "|   10050|   23|\n",
      "|   10040|   14|\n",
      "|   10017|   40|\n",
      "|    9901|   15|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "venda_loja = sqlContext.sql('SELECT COUNT(1), store FROM sales GROUP BY store ORDER BY 1 desc')\n",
    "venda_loja.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|dept|\n",
      "+----+\n",
      "|  26|\n",
      "|  29|\n",
      "|  65|\n",
      "|  19|\n",
      "|  54|\n",
      "|  22|\n",
      "|   7|\n",
      "|  77|\n",
      "|  34|\n",
      "|  94|\n",
      "|  50|\n",
      "|  32|\n",
      "|  43|\n",
      "|  31|\n",
      "|  98|\n",
      "|  39|\n",
      "|  25|\n",
      "|  95|\n",
      "|  71|\n",
      "|   6|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = sqlContext.sql ('SELECT DISTINCT dept FROM sales')\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|count(1)|dept|\n",
      "+--------+----+\n",
      "|      12|  43|\n",
      "|      16|  39|\n",
      "|     143|  65|\n",
      "|     150|  77|\n",
      "|     235|  78|\n",
      "|     646|  47|\n",
      "|     862|  99|\n",
      "|    1394|  51|\n",
      "|    1562|  50|\n",
      "|    1742|  48|\n",
      "|    1936|  45|\n",
      "|    2577|  37|\n",
      "|    4119|  19|\n",
      "|    4390|  58|\n",
      "|    4482|  49|\n",
      "|    4767|  54|\n",
      "|    4854|  96|\n",
      "|    5029|  18|\n",
      "|    5291|  30|\n",
      "|    5295|  36|\n",
      "+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_loja = sqlContext.sql ('SELECT COUNT(1), dept FROM sales GROUP BY dept ORDER BY 1')\n",
    "dept_loja.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|store|   MEDIA|\n",
      "+-----+--------+\n",
      "|   20| 29508.3|\n",
      "|    4|29161.21|\n",
      "|   14|28784.85|\n",
      "|   13|27355.14|\n",
      "|    2|26898.07|\n",
      "|   10| 26332.3|\n",
      "|   27|24826.98|\n",
      "|    6|21913.24|\n",
      "|    1|21710.54|\n",
      "|   39|21000.76|\n",
      "|   19|20362.13|\n",
      "|   23|19776.18|\n",
      "|   31|19681.91|\n",
      "|   11|19276.76|\n",
      "|   24|18969.11|\n",
      "|   28|18714.89|\n",
      "|   41| 17976.0|\n",
      "|   32|16351.62|\n",
      "|   18|15733.31|\n",
      "|   22|15181.22|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "media_loja = sqlContext.sql('SELECT store, ROUND(AVG(Weekly_Sales),2) AS MEDIA FROM sales GROUP BY store ORDER BY 2 DESC')\n",
    "media_loja.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)Referencias Consultadas:\n",
    "\n",
    "ADOBE. Adobe Experience League, 2022. Spark Funções SQL. Disponível em: https://experienceleague.adobe.com/docs/experience-platform/query/sql/spark-sql-functions.html?lang=pt-BR. Acesso em 15/04/2022.\n",
    "\n",
    "APACHE SPARK. Apache Spark, 2018. Spark SQL, Built-in Functions. Disponível em: https://spark.apache.org/docs/latest/api/sql/index.html. Acesso em 16/04/2022. \n",
    "\n",
    "DAMJI, J. et al. Learning Spark: Lightning-Fast Data Analytics. California: O’Reilly Media, 2020.\n",
    "\n",
    "DEVMEDIA. Devmedia, 2022. Introdução ao Apache Spark. Disponível em: https://www.devmedia.com.br/introducao-ao-apache-spark/34178. Acesso em 13/04/2022.\n",
    "\n",
    "DEVMEDIA. Devmedia, 2022. Apache Spark: Trabalhando com SQL em aplicações Big Data. Disponível em: https://www.devmedia.com.br/apache-spark-trabalhando-com-sql-em-aplicacoes-big-data/34251. Acesso em 15/04/2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
